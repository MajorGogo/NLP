{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161ebf96-7440-455e-bc37-a0c241eb4c9d",
   "metadata": {},
   "source": [
    "CORPUS-->DOCUMENT(LIST OF SENTENCES)--->SENTENCES---->TOKENIZER(TO EXTRACT EACH WORD)\n",
    "\n",
    "sent_tokenize---> EXTRACTS THE SENTENCES FROM THE DOCUMENT(LIST OF SENTENCES)\n",
    "word_tokenize----> EXTRACTS WORDS FROM THE COMPLETE CORPUS OR A SENTENCE BUT NOT FROM A DOCUMENT\n",
    "wordpunct_tokenize---->CONSIDERS ALL SPECIAL SYMBOLS AS SEPERATE ENTITY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab1e090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in d:\\new folder\\envs\\machine_learning\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\new folder\\envs\\machine_learning\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\new folder\\envs\\machine_learning\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in d:\\new folder\\envs\\machine_learning\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in d:\\new folder\\envs\\machine_learning\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 435.7 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.0/1.5 MB 487.6 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.1/1.5 MB 581.0 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.1/1.5 MB 552.2 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 701.4 kB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 701.4 kB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 701.4 kB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 436.9 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 461.0 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.3/1.5 MB 548.9 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.3/1.5 MB 615.9 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 730.1 kB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 688.1 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.5 MB 761.9 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.5 MB 761.9 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.5/1.5 MB 696.3 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.6/1.5 MB 745.2 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 764.4 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 0.7/1.5 MB 772.4 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 0.7/1.5 MB 731.4 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.7/1.5 MB 737.3 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.8/1.5 MB 783.9 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 795.6 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.5 MB 834.6 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 840.8 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 840.8 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 840.8 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 793.6 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 804.3 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.2/1.5 MB 822.4 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 822.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.5 MB 830.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 893.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 863.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 865.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 885.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 880.5 kB/s eta 0:00:00\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51710922",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"This brings me to the last of the big questions: the future of the human race. If we are the only intelligent beings in the galaxy, we should make sure we survive and continue. But we are entering an increasingly dangerous period of our history. Our population and our use of the finite resources of planet Earth are growing exponentially, along with our technical ability to change the environment for good or ill. But our genetic code still carries the selfish and aggressive instincts that were of survival advantage in the past. It will be difficult enough to avoid disaster in the next hundred years, let alone the next thousand or million.\n",
    "Our only chance of long-term survival is not to remain inward-looking on planet Earth, but to spread out into space. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6d5234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This brings me to the last of the big questions: the future of the human race. If we are the only intelligent beings in the galaxy, we should make sure we survive and continue. But we are entering an increasingly dangerous period of our history. Our population and our use of the finite resources of planet Earth are growing exponentially, along with our technical ability to change the environment for good or ill. But our genetic code still carries the selfish and aggressive instincts that were of survival advantage in the past. It will be difficult enough to avoid disaster in the next hundred years, let alone the next thousand or million.\n",
      "Our only chance of long-term survival is not to remain inward-looking on planet Earth, but to spread out into space. \n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a68812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7533c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus,language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd340978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This brings me to the last of the big questions: the future of the human race.', 'If we are the only intelligent beings in the galaxy, we should make sure we survive and continue.', 'But we are entering an increasingly dangerous period of our history.', 'Our population and our use of the finite resources of planet Earth are growing exponentially, along with our technical ability to change the environment for good or ill.', 'But our genetic code still carries the selfish and aggressive instincts that were of survival advantage in the past.', 'It will be difficult enough to avoid disaster in the next hundred years, let alone the next thousand or million.', 'Our only chance of long-term survival is not to remain inward-looking on planet Earth, but to spread out into space.']\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ccb86d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b8e6bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This brings me to the last of the big questions: the future of the human race.\n",
      "If we are the only intelligent beings in the galaxy, we should make sure we survive and continue.\n",
      "But we are entering an increasingly dangerous period of our history.\n",
      "Our population and our use of the finite resources of planet Earth are growing exponentially, along with our technical ability to change the environment for good or ill.\n",
      "But our genetic code still carries the selfish and aggressive instincts that were of survival advantage in the past.\n",
      "It will be difficult enough to avoid disaster in the next hundred years, let alone the next thousand or million.\n",
      "Our only chance of long-term survival is not to remain inward-looking on planet Earth, but to spread out into space.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bca3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b4cc04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'brings',\n",
       " 'me',\n",
       " 'to',\n",
       " 'the',\n",
       " 'last',\n",
       " 'of',\n",
       " 'the',\n",
       " 'big',\n",
       " 'questions',\n",
       " ':',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race',\n",
       " '.',\n",
       " 'If',\n",
       " 'we',\n",
       " 'are',\n",
       " 'the',\n",
       " 'only',\n",
       " 'intelligent',\n",
       " 'beings',\n",
       " 'in',\n",
       " 'the',\n",
       " 'galaxy',\n",
       " ',',\n",
       " 'we',\n",
       " 'should',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'we',\n",
       " 'survive',\n",
       " 'and',\n",
       " 'continue',\n",
       " '.',\n",
       " 'But',\n",
       " 'we',\n",
       " 'are',\n",
       " 'entering',\n",
       " 'an',\n",
       " 'increasingly',\n",
       " 'dangerous',\n",
       " 'period',\n",
       " 'of',\n",
       " 'our',\n",
       " 'history',\n",
       " '.',\n",
       " 'Our',\n",
       " 'population',\n",
       " 'and',\n",
       " 'our',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'finite',\n",
       " 'resources',\n",
       " 'of',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " 'are',\n",
       " 'growing',\n",
       " 'exponentially',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'our',\n",
       " 'technical',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'environment',\n",
       " 'for',\n",
       " 'good',\n",
       " 'or',\n",
       " 'ill',\n",
       " '.',\n",
       " 'But',\n",
       " 'our',\n",
       " 'genetic',\n",
       " 'code',\n",
       " 'still',\n",
       " 'carries',\n",
       " 'the',\n",
       " 'selfish',\n",
       " 'and',\n",
       " 'aggressive',\n",
       " 'instincts',\n",
       " 'that',\n",
       " 'were',\n",
       " 'of',\n",
       " 'survival',\n",
       " 'advantage',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'be',\n",
       " 'difficult',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'disaster',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'hundred',\n",
       " 'years',\n",
       " ',',\n",
       " 'let',\n",
       " 'alone',\n",
       " 'the',\n",
       " 'next',\n",
       " 'thousand',\n",
       " 'or',\n",
       " 'million',\n",
       " '.',\n",
       " 'Our',\n",
       " 'only',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'long-term',\n",
       " 'survival',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'remain',\n",
       " 'inward-looking',\n",
       " 'on',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " ',',\n",
       " 'but',\n",
       " 'to',\n",
       " 'spread',\n",
       " 'out',\n",
       " 'into',\n",
       " 'space',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98254332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'brings', 'me', 'to', 'the', 'last', 'of', 'the', 'big', 'questions', ':', 'the', 'future', 'of', 'the', 'human', 'race', '.']\n",
      "['If', 'we', 'are', 'the', 'only', 'intelligent', 'beings', 'in', 'the', 'galaxy', ',', 'we', 'should', 'make', 'sure', 'we', 'survive', 'and', 'continue', '.']\n",
      "['But', 'we', 'are', 'entering', 'an', 'increasingly', 'dangerous', 'period', 'of', 'our', 'history', '.']\n",
      "['Our', 'population', 'and', 'our', 'use', 'of', 'the', 'finite', 'resources', 'of', 'planet', 'Earth', 'are', 'growing', 'exponentially', ',', 'along', 'with', 'our', 'technical', 'ability', 'to', 'change', 'the', 'environment', 'for', 'good', 'or', 'ill', '.']\n",
      "['But', 'our', 'genetic', 'code', 'still', 'carries', 'the', 'selfish', 'and', 'aggressive', 'instincts', 'that', 'were', 'of', 'survival', 'advantage', 'in', 'the', 'past', '.']\n",
      "['It', 'will', 'be', 'difficult', 'enough', 'to', 'avoid', 'disaster', 'in', 'the', 'next', 'hundred', 'years', ',', 'let', 'alone', 'the', 'next', 'thousand', 'or', 'million', '.']\n",
      "['Our', 'only', 'chance', 'of', 'long-term', 'survival', 'is', 'not', 'to', 'remain', 'inward-looking', 'on', 'planet', 'Earth', ',', 'but', 'to', 'spread', 'out', 'into', 'space', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5795cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2457d5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'brings',\n",
       " 'me',\n",
       " 'to',\n",
       " 'the',\n",
       " 'last',\n",
       " 'of',\n",
       " 'the',\n",
       " 'big',\n",
       " 'questions',\n",
       " ':',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race',\n",
       " '.',\n",
       " 'If',\n",
       " 'we',\n",
       " 'are',\n",
       " 'the',\n",
       " 'only',\n",
       " 'intelligent',\n",
       " 'beings',\n",
       " 'in',\n",
       " 'the',\n",
       " 'galaxy',\n",
       " ',',\n",
       " 'we',\n",
       " 'should',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'we',\n",
       " 'survive',\n",
       " 'and',\n",
       " 'continue',\n",
       " '.',\n",
       " 'But',\n",
       " 'we',\n",
       " 'are',\n",
       " 'entering',\n",
       " 'an',\n",
       " 'increasingly',\n",
       " 'dangerous',\n",
       " 'period',\n",
       " 'of',\n",
       " 'our',\n",
       " 'history',\n",
       " '.',\n",
       " 'Our',\n",
       " 'population',\n",
       " 'and',\n",
       " 'our',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'finite',\n",
       " 'resources',\n",
       " 'of',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " 'are',\n",
       " 'growing',\n",
       " 'exponentially',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'our',\n",
       " 'technical',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'environment',\n",
       " 'for',\n",
       " 'good',\n",
       " 'or',\n",
       " 'ill',\n",
       " '.',\n",
       " 'But',\n",
       " 'our',\n",
       " 'genetic',\n",
       " 'code',\n",
       " 'still',\n",
       " 'carries',\n",
       " 'the',\n",
       " 'selfish',\n",
       " 'and',\n",
       " 'aggressive',\n",
       " 'instincts',\n",
       " 'that',\n",
       " 'were',\n",
       " 'of',\n",
       " 'survival',\n",
       " 'advantage',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'be',\n",
       " 'difficult',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'disaster',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'hundred',\n",
       " 'years',\n",
       " ',',\n",
       " 'let',\n",
       " 'alone',\n",
       " 'the',\n",
       " 'next',\n",
       " 'thousand',\n",
       " 'or',\n",
       " 'million',\n",
       " '.',\n",
       " 'Our',\n",
       " 'only',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'long',\n",
       " '-',\n",
       " 'term',\n",
       " 'survival',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'remain',\n",
       " 'inward',\n",
       " '-',\n",
       " 'looking',\n",
       " 'on',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " ',',\n",
       " 'but',\n",
       " 'to',\n",
       " 'spread',\n",
       " 'out',\n",
       " 'into',\n",
       " 'space',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6673b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d4ecaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc20e5a-8606-445c-b06b-f8d83a760faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'brings',\n",
       " 'me',\n",
       " 'to',\n",
       " 'the',\n",
       " 'last',\n",
       " 'of',\n",
       " 'the',\n",
       " 'big',\n",
       " 'questions',\n",
       " ':',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race.',\n",
       " 'If',\n",
       " 'we',\n",
       " 'are',\n",
       " 'the',\n",
       " 'only',\n",
       " 'intelligent',\n",
       " 'beings',\n",
       " 'in',\n",
       " 'the',\n",
       " 'galaxy',\n",
       " ',',\n",
       " 'we',\n",
       " 'should',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'we',\n",
       " 'survive',\n",
       " 'and',\n",
       " 'continue.',\n",
       " 'But',\n",
       " 'we',\n",
       " 'are',\n",
       " 'entering',\n",
       " 'an',\n",
       " 'increasingly',\n",
       " 'dangerous',\n",
       " 'period',\n",
       " 'of',\n",
       " 'our',\n",
       " 'history.',\n",
       " 'Our',\n",
       " 'population',\n",
       " 'and',\n",
       " 'our',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'finite',\n",
       " 'resources',\n",
       " 'of',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " 'are',\n",
       " 'growing',\n",
       " 'exponentially',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'our',\n",
       " 'technical',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'environment',\n",
       " 'for',\n",
       " 'good',\n",
       " 'or',\n",
       " 'ill.',\n",
       " 'But',\n",
       " 'our',\n",
       " 'genetic',\n",
       " 'code',\n",
       " 'still',\n",
       " 'carries',\n",
       " 'the',\n",
       " 'selfish',\n",
       " 'and',\n",
       " 'aggressive',\n",
       " 'instincts',\n",
       " 'that',\n",
       " 'were',\n",
       " 'of',\n",
       " 'survival',\n",
       " 'advantage',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'be',\n",
       " 'difficult',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'disaster',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'hundred',\n",
       " 'years',\n",
       " ',',\n",
       " 'let',\n",
       " 'alone',\n",
       " 'the',\n",
       " 'next',\n",
       " 'thousand',\n",
       " 'or',\n",
       " 'million.',\n",
       " 'Our',\n",
       " 'only',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'long-term',\n",
       " 'survival',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'remain',\n",
       " 'inward-looking',\n",
       " 'on',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " ',',\n",
       " 'but',\n",
       " 'to',\n",
       " 'spread',\n",
       " 'out',\n",
       " 'into',\n",
       " 'space',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc649f-2c3b-4a1c-82a5-e90f8838f414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
