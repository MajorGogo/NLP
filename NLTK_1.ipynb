{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292cb001-a3d6-4772-a8fa-e1e94099dc35",
   "metadata": {},
   "source": [
    "CORPUS-->DOCUMENT(LIST OF SENTENCES)--->SENTENCES---->TOKENIZER(TO EXTRACT EACH WORD)\n",
    "\n",
    "sent_tokenize---> EXTRACTS THE SENTENCES FROM THE DOCUMENT(LIST OF SENTENCES)\n",
    "word_tokenize----> EXTRACTS WORDS FROM THE COMPLETE CORPUS OR A SENTENCE BUT NOT FROM A DOCUMENT\n",
    "wordpunct_tokenize---->CONSIDERS ALL SPECIAL SYMBOLS AS SEPERATE ENTITY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab1e090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\new folder\\envs\\machine_learning\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\new folder\\envs\\machine_learning\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\new folder\\envs\\machine_learning\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\new folder\\envs\\machine_learning\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in d:\\new folder\\envs\\machine_learning\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in d:\\new folder\\envs\\machine_learning\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51710922",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"This brings me to the last of the big questions: the future of the human race. If we are the only intelligent beings in the galaxy, we should make sure we survive and continue. But we are entering an increasingly dangerous period of our history. Our population and our use of the finite resources of planet Earth are growing exponentially, along with our technical ability to change the environment for good or ill. But our genetic code still carries the selfish and aggressive instincts that were of survival advantage in the past. It will be difficult enough to avoid disaster in the next hundred years, let alone the next thousand or million.\n",
    "Our only chance of long-term survival is not to remain inward-looking on planet Earth, but to spread out into space. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6d5234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This brings me to the last of the big questions: the future of the human race. If we are the only intelligent beings in the galaxy, we should make sure we survive and continue. But we are entering an increasingly dangerous period of our history. Our population and our use of the finite resources of planet Earth are growing exponentially, along with our technical ability to change the environment for good or ill. But our genetic code still carries the selfish and aggressive instincts that were of survival advantage in the past. It will be difficult enough to avoid disaster in the next hundred years, let alone the next thousand or million.\n",
      "Our only chance of long-term survival is not to remain inward-looking on planet Earth, but to spread out into space. \n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a68812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7533c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus,language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd340978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This brings me to the last of the big questions: the future of the human race.', 'If we are the only intelligent beings in the galaxy, we should make sure we survive and continue.', 'But we are entering an increasingly dangerous period of our history.', 'Our population and our use of the finite resources of planet Earth are growing exponentially, along with our technical ability to change the environment for good or ill.', 'But our genetic code still carries the selfish and aggressive instincts that were of survival advantage in the past.', 'It will be difficult enough to avoid disaster in the next hundred years, let alone the next thousand or million.', 'Our only chance of long-term survival is not to remain inward-looking on planet Earth, but to spread out into space.']\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ccb86d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8e6bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This brings me to the last of the big questions: the future of the human race.\n",
      "If we are the only intelligent beings in the galaxy, we should make sure we survive and continue.\n",
      "But we are entering an increasingly dangerous period of our history.\n",
      "Our population and our use of the finite resources of planet Earth are growing exponentially, along with our technical ability to change the environment for good or ill.\n",
      "But our genetic code still carries the selfish and aggressive instincts that were of survival advantage in the past.\n",
      "It will be difficult enough to avoid disaster in the next hundred years, let alone the next thousand or million.\n",
      "Our only chance of long-term survival is not to remain inward-looking on planet Earth, but to spread out into space.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bca3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4cc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98254332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'brings', 'me', 'to', 'the', 'last', 'of', 'the', 'big', 'questions', ':', 'the', 'future', 'of', 'the', 'human', 'race', '.']\n",
      "['If', 'we', 'are', 'the', 'only', 'intelligent', 'beings', 'in', 'the', 'galaxy', ',', 'we', 'should', 'make', 'sure', 'we', 'survive', 'and', 'continue', '.']\n",
      "['But', 'we', 'are', 'entering', 'an', 'increasingly', 'dangerous', 'period', 'of', 'our', 'history', '.']\n",
      "['Our', 'population', 'and', 'our', 'use', 'of', 'the', 'finite', 'resources', 'of', 'planet', 'Earth', 'are', 'growing', 'exponentially', ',', 'along', 'with', 'our', 'technical', 'ability', 'to', 'change', 'the', 'environment', 'for', 'good', 'or', 'ill', '.']\n",
      "['But', 'our', 'genetic', 'code', 'still', 'carries', 'the', 'selfish', 'and', 'aggressive', 'instincts', 'that', 'were', 'of', 'survival', 'advantage', 'in', 'the', 'past', '.']\n",
      "['It', 'will', 'be', 'difficult', 'enough', 'to', 'avoid', 'disaster', 'in', 'the', 'next', 'hundred', 'years', ',', 'let', 'alone', 'the', 'next', 'thousand', 'or', 'million', '.']\n",
      "['Our', 'only', 'chance', 'of', 'long-term', 'survival', 'is', 'not', 'to', 'remain', 'inward-looking', 'on', 'planet', 'Earth', ',', 'but', 'to', 'spread', 'out', 'into', 'space', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5795cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2457d5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'brings',\n",
       " 'me',\n",
       " 'to',\n",
       " 'the',\n",
       " 'last',\n",
       " 'of',\n",
       " 'the',\n",
       " 'big',\n",
       " 'questions',\n",
       " ':',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race',\n",
       " '.',\n",
       " 'If',\n",
       " 'we',\n",
       " 'are',\n",
       " 'the',\n",
       " 'only',\n",
       " 'intelligent',\n",
       " 'beings',\n",
       " 'in',\n",
       " 'the',\n",
       " 'galaxy',\n",
       " ',',\n",
       " 'we',\n",
       " 'should',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'we',\n",
       " 'survive',\n",
       " 'and',\n",
       " 'continue',\n",
       " '.',\n",
       " 'But',\n",
       " 'we',\n",
       " 'are',\n",
       " 'entering',\n",
       " 'an',\n",
       " 'increasingly',\n",
       " 'dangerous',\n",
       " 'period',\n",
       " 'of',\n",
       " 'our',\n",
       " 'history',\n",
       " '.',\n",
       " 'Our',\n",
       " 'population',\n",
       " 'and',\n",
       " 'our',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'finite',\n",
       " 'resources',\n",
       " 'of',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " 'are',\n",
       " 'growing',\n",
       " 'exponentially',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'our',\n",
       " 'technical',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'environment',\n",
       " 'for',\n",
       " 'good',\n",
       " 'or',\n",
       " 'ill',\n",
       " '.',\n",
       " 'But',\n",
       " 'our',\n",
       " 'genetic',\n",
       " 'code',\n",
       " 'still',\n",
       " 'carries',\n",
       " 'the',\n",
       " 'selfish',\n",
       " 'and',\n",
       " 'aggressive',\n",
       " 'instincts',\n",
       " 'that',\n",
       " 'were',\n",
       " 'of',\n",
       " 'survival',\n",
       " 'advantage',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'be',\n",
       " 'difficult',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'disaster',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'hundred',\n",
       " 'years',\n",
       " ',',\n",
       " 'let',\n",
       " 'alone',\n",
       " 'the',\n",
       " 'next',\n",
       " 'thousand',\n",
       " 'or',\n",
       " 'million',\n",
       " '.',\n",
       " 'Our',\n",
       " 'only',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'long',\n",
       " '-',\n",
       " 'term',\n",
       " 'survival',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'remain',\n",
       " 'inward',\n",
       " '-',\n",
       " 'looking',\n",
       " 'on',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " ',',\n",
       " 'but',\n",
       " 'to',\n",
       " 'spread',\n",
       " 'out',\n",
       " 'into',\n",
       " 'space',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6673b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d4ecaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afc20e5a-8606-445c-b06b-f8d83a760faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'brings',\n",
       " 'me',\n",
       " 'to',\n",
       " 'the',\n",
       " 'last',\n",
       " 'of',\n",
       " 'the',\n",
       " 'big',\n",
       " 'questions',\n",
       " ':',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race.',\n",
       " 'If',\n",
       " 'we',\n",
       " 'are',\n",
       " 'the',\n",
       " 'only',\n",
       " 'intelligent',\n",
       " 'beings',\n",
       " 'in',\n",
       " 'the',\n",
       " 'galaxy',\n",
       " ',',\n",
       " 'we',\n",
       " 'should',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'we',\n",
       " 'survive',\n",
       " 'and',\n",
       " 'continue.',\n",
       " 'But',\n",
       " 'we',\n",
       " 'are',\n",
       " 'entering',\n",
       " 'an',\n",
       " 'increasingly',\n",
       " 'dangerous',\n",
       " 'period',\n",
       " 'of',\n",
       " 'our',\n",
       " 'history.',\n",
       " 'Our',\n",
       " 'population',\n",
       " 'and',\n",
       " 'our',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'finite',\n",
       " 'resources',\n",
       " 'of',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " 'are',\n",
       " 'growing',\n",
       " 'exponentially',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'our',\n",
       " 'technical',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'environment',\n",
       " 'for',\n",
       " 'good',\n",
       " 'or',\n",
       " 'ill.',\n",
       " 'But',\n",
       " 'our',\n",
       " 'genetic',\n",
       " 'code',\n",
       " 'still',\n",
       " 'carries',\n",
       " 'the',\n",
       " 'selfish',\n",
       " 'and',\n",
       " 'aggressive',\n",
       " 'instincts',\n",
       " 'that',\n",
       " 'were',\n",
       " 'of',\n",
       " 'survival',\n",
       " 'advantage',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'be',\n",
       " 'difficult',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'disaster',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'hundred',\n",
       " 'years',\n",
       " ',',\n",
       " 'let',\n",
       " 'alone',\n",
       " 'the',\n",
       " 'next',\n",
       " 'thousand',\n",
       " 'or',\n",
       " 'million.',\n",
       " 'Our',\n",
       " 'only',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'long-term',\n",
       " 'survival',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'remain',\n",
       " 'inward-looking',\n",
       " 'on',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " ',',\n",
       " 'but',\n",
       " 'to',\n",
       " 'spread',\n",
       " 'out',\n",
       " 'into',\n",
       " 'space',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7dc649f-2c3b-4a1c-82a5-e90f8838f414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'brings',\n",
       " 'me',\n",
       " 'to',\n",
       " 'the',\n",
       " 'last',\n",
       " 'of',\n",
       " 'the',\n",
       " 'big',\n",
       " 'questions',\n",
       " ':',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race',\n",
       " '.',\n",
       " 'If',\n",
       " 'we',\n",
       " 'are',\n",
       " 'the',\n",
       " 'only',\n",
       " 'intelligent',\n",
       " 'beings',\n",
       " 'in',\n",
       " 'the',\n",
       " 'galaxy',\n",
       " ',',\n",
       " 'we',\n",
       " 'should',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'we',\n",
       " 'survive',\n",
       " 'and',\n",
       " 'continue',\n",
       " '.',\n",
       " 'But',\n",
       " 'we',\n",
       " 'are',\n",
       " 'entering',\n",
       " 'an',\n",
       " 'increasingly',\n",
       " 'dangerous',\n",
       " 'period',\n",
       " 'of',\n",
       " 'our',\n",
       " 'history',\n",
       " '.',\n",
       " 'Our',\n",
       " 'population',\n",
       " 'and',\n",
       " 'our',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'finite',\n",
       " 'resources',\n",
       " 'of',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " 'are',\n",
       " 'growing',\n",
       " 'exponentially',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'our',\n",
       " 'technical',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'environment',\n",
       " 'for',\n",
       " 'good',\n",
       " 'or',\n",
       " 'ill',\n",
       " '.',\n",
       " 'But',\n",
       " 'our',\n",
       " 'genetic',\n",
       " 'code',\n",
       " 'still',\n",
       " 'carries',\n",
       " 'the',\n",
       " 'selfish',\n",
       " 'and',\n",
       " 'aggressive',\n",
       " 'instincts',\n",
       " 'that',\n",
       " 'were',\n",
       " 'of',\n",
       " 'survival',\n",
       " 'advantage',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'be',\n",
       " 'difficult',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'disaster',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'hundred',\n",
       " 'years',\n",
       " ',',\n",
       " 'let',\n",
       " 'alone',\n",
       " 'the',\n",
       " 'next',\n",
       " 'thousand',\n",
       " 'or',\n",
       " 'million',\n",
       " '.',\n",
       " 'Our',\n",
       " 'only',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'long-term',\n",
       " 'survival',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'remain',\n",
       " 'inward-looking',\n",
       " 'on',\n",
       " 'planet',\n",
       " 'Earth',\n",
       " ',',\n",
       " 'but',\n",
       " 'to',\n",
       " 'spread',\n",
       " 'out',\n",
       " 'into',\n",
       " 'space',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fb31638-b7e5-4a0d-83b5-a5323ad682d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This---->thi\n",
      "brings---->bring\n",
      "me---->me\n",
      "to---->to\n",
      "the---->the\n",
      "last---->last\n",
      "of---->of\n",
      "the---->the\n",
      "big---->big\n",
      "questions---->question\n",
      ":---->:\n",
      "the---->the\n",
      "future---->futur\n",
      "of---->of\n",
      "the---->the\n",
      "human---->human\n",
      "race---->race\n",
      ".---->.\n",
      "If---->if\n",
      "we---->we\n",
      "are---->are\n",
      "the---->the\n",
      "only---->onli\n",
      "intelligent---->intellig\n",
      "beings---->be\n",
      "in---->in\n",
      "the---->the\n",
      "galaxy---->galaxi\n",
      ",---->,\n",
      "we---->we\n",
      "should---->should\n",
      "make---->make\n",
      "sure---->sure\n",
      "we---->we\n",
      "survive---->surviv\n",
      "and---->and\n",
      "continue---->continu\n",
      ".---->.\n",
      "But---->but\n",
      "we---->we\n",
      "are---->are\n",
      "entering---->enter\n",
      "an---->an\n",
      "increasingly---->increasingli\n",
      "dangerous---->danger\n",
      "period---->period\n",
      "of---->of\n",
      "our---->our\n",
      "history---->histori\n",
      ".---->.\n",
      "Our---->our\n",
      "population---->popul\n",
      "and---->and\n",
      "our---->our\n",
      "use---->use\n",
      "of---->of\n",
      "the---->the\n",
      "finite---->finit\n",
      "resources---->resourc\n",
      "of---->of\n",
      "planet---->planet\n",
      "Earth---->earth\n",
      "are---->are\n",
      "growing---->grow\n",
      "exponentially---->exponenti\n",
      ",---->,\n",
      "along---->along\n",
      "with---->with\n",
      "our---->our\n",
      "technical---->technic\n",
      "ability---->abil\n",
      "to---->to\n",
      "change---->chang\n",
      "the---->the\n",
      "environment---->environ\n",
      "for---->for\n",
      "good---->good\n",
      "or---->or\n",
      "ill---->ill\n",
      ".---->.\n",
      "But---->but\n",
      "our---->our\n",
      "genetic---->genet\n",
      "code---->code\n",
      "still---->still\n",
      "carries---->carri\n",
      "the---->the\n",
      "selfish---->selfish\n",
      "and---->and\n",
      "aggressive---->aggress\n",
      "instincts---->instinct\n",
      "that---->that\n",
      "were---->were\n",
      "of---->of\n",
      "survival---->surviv\n",
      "advantage---->advantag\n",
      "in---->in\n",
      "the---->the\n",
      "past---->past\n",
      ".---->.\n",
      "It---->it\n",
      "will---->will\n",
      "be---->be\n",
      "difficult---->difficult\n",
      "enough---->enough\n",
      "to---->to\n",
      "avoid---->avoid\n",
      "disaster---->disast\n",
      "in---->in\n",
      "the---->the\n",
      "next---->next\n",
      "hundred---->hundr\n",
      "years---->year\n",
      ",---->,\n",
      "let---->let\n",
      "alone---->alon\n",
      "the---->the\n",
      "next---->next\n",
      "thousand---->thousand\n",
      "or---->or\n",
      "million---->million\n",
      ".---->.\n",
      "Our---->our\n",
      "only---->onli\n",
      "chance---->chanc\n",
      "of---->of\n",
      "long-term---->long-term\n",
      "survival---->surviv\n",
      "is---->is\n",
      "not---->not\n",
      "to---->to\n",
      "remain---->remain\n",
      "inward-looking---->inward-look\n",
      "on---->on\n",
      "planet---->planet\n",
      "Earth---->earth\n",
      ",---->,\n",
      "but---->but\n",
      "to---->to\n",
      "spread---->spread\n",
      "out---->out\n",
      "into---->into\n",
      "space---->space\n",
      ".---->.\n"
     ]
    }
   ],
   "source": [
    "# PORTERSTEMMER\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming=PorterStemmer()\n",
    "for word in words:\n",
    "    print(word+\"---->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deea0333-a637-49a7-8359-c49161d1dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpStemmer class\n",
    "# NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. \n",
    "# It basically takes a single regular expression and removes any prefix or suffix that matches the expression.\n",
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cd75fe3-0e83-4511-9583-acf7c6c75b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fded729-9347-468a-afa7-575727ef1623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNABLE TO REMOVE THE PREFIX\n",
    "reg_stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8911dc09-2d80-41b1-a57d-7839698923d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This---->this\n",
      "brings---->bring\n",
      "me---->me\n",
      "to---->to\n",
      "the---->the\n",
      "last---->last\n",
      "of---->of\n",
      "the---->the\n",
      "big---->big\n",
      "questions---->question\n",
      ":---->:\n",
      "the---->the\n",
      "future---->futur\n",
      "of---->of\n",
      "the---->the\n",
      "human---->human\n",
      "race---->race\n",
      ".---->.\n",
      "If---->if\n",
      "we---->we\n",
      "are---->are\n",
      "the---->the\n",
      "only---->onli\n",
      "intelligent---->intellig\n",
      "beings---->be\n",
      "in---->in\n",
      "the---->the\n",
      "galaxy---->galaxi\n",
      ",---->,\n",
      "we---->we\n",
      "should---->should\n",
      "make---->make\n",
      "sure---->sure\n",
      "we---->we\n",
      "survive---->surviv\n",
      "and---->and\n",
      "continue---->continu\n",
      ".---->.\n",
      "But---->but\n",
      "we---->we\n",
      "are---->are\n",
      "entering---->enter\n",
      "an---->an\n",
      "increasingly---->increas\n",
      "dangerous---->danger\n",
      "period---->period\n",
      "of---->of\n",
      "our---->our\n",
      "history---->histori\n",
      ".---->.\n",
      "Our---->our\n",
      "population---->popul\n",
      "and---->and\n",
      "our---->our\n",
      "use---->use\n",
      "of---->of\n",
      "the---->the\n",
      "finite---->finit\n",
      "resources---->resourc\n",
      "of---->of\n",
      "planet---->planet\n",
      "Earth---->earth\n",
      "are---->are\n",
      "growing---->grow\n",
      "exponentially---->exponenti\n",
      ",---->,\n",
      "along---->along\n",
      "with---->with\n",
      "our---->our\n",
      "technical---->technic\n",
      "ability---->abil\n",
      "to---->to\n",
      "change---->chang\n",
      "the---->the\n",
      "environment---->environ\n",
      "for---->for\n",
      "good---->good\n",
      "or---->or\n",
      "ill---->ill\n",
      ".---->.\n",
      "But---->but\n",
      "our---->our\n",
      "genetic---->genet\n",
      "code---->code\n",
      "still---->still\n",
      "carries---->carri\n",
      "the---->the\n",
      "selfish---->selfish\n",
      "and---->and\n",
      "aggressive---->aggress\n",
      "instincts---->instinct\n",
      "that---->that\n",
      "were---->were\n",
      "of---->of\n",
      "survival---->surviv\n",
      "advantage---->advantag\n",
      "in---->in\n",
      "the---->the\n",
      "past---->past\n",
      ".---->.\n",
      "It---->it\n",
      "will---->will\n",
      "be---->be\n",
      "difficult---->difficult\n",
      "enough---->enough\n",
      "to---->to\n",
      "avoid---->avoid\n",
      "disaster---->disast\n",
      "in---->in\n",
      "the---->the\n",
      "next---->next\n",
      "hundred---->hundr\n",
      "years---->year\n",
      ",---->,\n",
      "let---->let\n",
      "alone---->alon\n",
      "the---->the\n",
      "next---->next\n",
      "thousand---->thousand\n",
      "or---->or\n",
      "million---->million\n",
      ".---->.\n",
      "Our---->our\n",
      "only---->onli\n",
      "chance---->chanc\n",
      "of---->of\n",
      "long-term---->long-term\n",
      "survival---->surviv\n",
      "is---->is\n",
      "not---->not\n",
      "to---->to\n",
      "remain---->remain\n",
      "inward-looking---->inward-look\n",
      "on---->on\n",
      "planet---->planet\n",
      "Earth---->earth\n",
      ",---->,\n",
      "but---->but\n",
      "to---->to\n",
      "spread---->spread\n",
      "out---->out\n",
      "into---->into\n",
      "space---->space\n",
      ".---->.\n"
     ]
    }
   ],
   "source": [
    "#SnowballStemmer(BETTER VERSION OF PORTER STEMMER)\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer=SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word+\"---->\"+snowballsstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28627ff-5357-4d1c-b203-4a2db924d870",
   "metadata": {},
   "source": [
    "Wordnet Lemmatizer\n",
    "Lemmatization technique is like stemming. \n",
    "The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. \n",
    "After lemmatization, we will be getting a valid word that means the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2efd7c3e-7e51-46d3-a4bc-819dde71c063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPOS- Noun-n\\nverb-v\\nadjective-a\\nadverb-r\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "'''\n",
    "POS- Noun-n\n",
    "verb-v\n",
    "adjective-a\n",
    "adverb-r\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd96f90c-4468-4a06-85aa-ce415f62f6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2381a123-e921-456d-8825-a8cd31213533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This---->This\n",
      "brings---->bring\n",
      "me---->me\n",
      "to---->to\n",
      "the---->the\n",
      "last---->last\n",
      "of---->of\n",
      "the---->the\n",
      "big---->big\n",
      "questions---->question\n",
      ":---->:\n",
      "the---->the\n",
      "future---->future\n",
      "of---->of\n",
      "the---->the\n",
      "human---->human\n",
      "race---->race\n",
      ".---->.\n",
      "If---->If\n",
      "we---->we\n",
      "are---->be\n",
      "the---->the\n",
      "only---->only\n",
      "intelligent---->intelligent\n",
      "beings---->be\n",
      "in---->in\n",
      "the---->the\n",
      "galaxy---->galaxy\n",
      ",---->,\n",
      "we---->we\n",
      "should---->should\n",
      "make---->make\n",
      "sure---->sure\n",
      "we---->we\n",
      "survive---->survive\n",
      "and---->and\n",
      "continue---->continue\n",
      ".---->.\n",
      "But---->But\n",
      "we---->we\n",
      "are---->be\n",
      "entering---->enter\n",
      "an---->an\n",
      "increasingly---->increasingly\n",
      "dangerous---->dangerous\n",
      "period---->period\n",
      "of---->of\n",
      "our---->our\n",
      "history---->history\n",
      ".---->.\n",
      "Our---->Our\n",
      "population---->population\n",
      "and---->and\n",
      "our---->our\n",
      "use---->use\n",
      "of---->of\n",
      "the---->the\n",
      "finite---->finite\n",
      "resources---->resources\n",
      "of---->of\n",
      "planet---->planet\n",
      "Earth---->Earth\n",
      "are---->be\n",
      "growing---->grow\n",
      "exponentially---->exponentially\n",
      ",---->,\n",
      "along---->along\n",
      "with---->with\n",
      "our---->our\n",
      "technical---->technical\n",
      "ability---->ability\n",
      "to---->to\n",
      "change---->change\n",
      "the---->the\n",
      "environment---->environment\n",
      "for---->for\n",
      "good---->good\n",
      "or---->or\n",
      "ill---->ill\n",
      ".---->.\n",
      "But---->But\n",
      "our---->our\n",
      "genetic---->genetic\n",
      "code---->code\n",
      "still---->still\n",
      "carries---->carry\n",
      "the---->the\n",
      "selfish---->selfish\n",
      "and---->and\n",
      "aggressive---->aggressive\n",
      "instincts---->instincts\n",
      "that---->that\n",
      "were---->be\n",
      "of---->of\n",
      "survival---->survival\n",
      "advantage---->advantage\n",
      "in---->in\n",
      "the---->the\n",
      "past---->past\n",
      ".---->.\n",
      "It---->It\n",
      "will---->will\n",
      "be---->be\n",
      "difficult---->difficult\n",
      "enough---->enough\n",
      "to---->to\n",
      "avoid---->avoid\n",
      "disaster---->disaster\n",
      "in---->in\n",
      "the---->the\n",
      "next---->next\n",
      "hundred---->hundred\n",
      "years---->years\n",
      ",---->,\n",
      "let---->let\n",
      "alone---->alone\n",
      "the---->the\n",
      "next---->next\n",
      "thousand---->thousand\n",
      "or---->or\n",
      "million---->million\n",
      ".---->.\n",
      "Our---->Our\n",
      "only---->only\n",
      "chance---->chance\n",
      "of---->of\n",
      "long-term---->long-term\n",
      "survival---->survival\n",
      "is---->be\n",
      "not---->not\n",
      "to---->to\n",
      "remain---->remain\n",
      "inward-looking---->inward-looking\n",
      "on---->on\n",
      "planet---->planet\n",
      "Earth---->Earth\n",
      ",---->,\n",
      "but---->but\n",
      "to---->to\n",
      "spread---->spread\n",
      "out---->out\n",
      "into---->into\n",
      "space---->space\n",
      ".---->.\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353e22c-7e8b-4613-a5e5-c8df748459a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
